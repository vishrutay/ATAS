{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bef094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave, os, glob\n",
    "import noisereduce\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import sys\n",
    "import numpy\n",
    "import librosa\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "from pydub import AudioSegment,silence\n",
    "import math\n",
    "from scipy.stats import variation \n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4dc600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all parameters file \n",
    "par_details = pd.read_csv('.../Stat_csv_files/AWNS_AWS_all_details.csv')# Change path as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d19269",
   "metadata": {},
   "outputs": [],
   "source": [
    "par_details['All_events_durations'] = par_details['All_events_durations'].apply(lambda x: np.fromstring(x, sep=','))\n",
    "par_details['long_p_durations'] = par_details['long_p_durations'].apply(lambda x: np.fromstring(x, sep=','))\n",
    "par_details['short_p_durations'] = par_details['short_p_durations'].apply(lambda x: np.fromstring(x, sep=','))\n",
    "par_details['Event_type'] = par_details['Event_type'].apply(lambda x: np.fromstring(x, sep=','))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7521c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_f_pre_nor = par_details[['All_events_durations','Total_Duration_Clipped_s','Speech_Duration_s','Pause_Duration_s','Speech_Events','Pause_Events','CV Speech','Mean Pause_s','long_p_count','short_p_count','Speech_Rate','long_p_durations_cv','short_p_durations_mean','Event_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae488f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_f_pre_nor_copy = df_all_f_pre_nor.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_column = par_details['Group']\n",
    "numerical_labels = np.array([0 if label == \"AWS\" else 1 for label in output_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572bb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_f_pre_nor_copy['Numerical_Label'] = numerical_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b0638",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabcd86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all columns except 'All_events_durations', 'Event_type', and 'Numerical_Label'\n",
    "columns_to_normalize = df_all_f_pre_nor_copy.columns.difference(['All_events_durations', 'Event_type', 'Numerical_Label'])\n",
    "df_all_f_pre_nor_copy[columns_to_normalize] = scaler.fit_transform(df_all_f_pre_nor_copy[columns_to_normalize])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64864da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables to store compiled data\n",
    "input_data_2_segs = []\n",
    "new_less = []\n",
    "data_frames = []    # For 'Numerical_Label' == 1\n",
    "data_frames_1 = []  # For 'Numerical_Label' == 0\n",
    "data_frames_less = []\n",
    "\n",
    "# Separate segments based on 'Numerical_Label' values\n",
    "input_data_2_segs_1 = []  # For 'Numerical_Label' == 1\n",
    "input_data_2_segs_2 = []  # For 'Numerical_Label' == 0\n",
    "\n",
    "# Iterate over each row in df_all_f_pre_nor_copy to process event data\n",
    "for idx, row in df_all_f_pre_nor_copy.iterrows():\n",
    "    # Extract 'All_event_durations' as 'Time_diff' and 'Event_type' directly from the row\n",
    "    input_data_2_seg = pd.DataFrame({'Time_diff': row['All_events_durations']})\n",
    "    input_data_2_seg['Event_type'] = row['Event_type']\n",
    "    \n",
    "    # Normalize 'Time_diff' in the input segment\n",
    "    input_data_2_seg['Time_diff'] = scaler.fit_transform(input_data_2_seg[['Time_diff']])\n",
    "    \n",
    "    #print(input_data_2_seg)\n",
    "    # Repeat the other participant details for each event in the row\n",
    "    row_df_use = row.drop(['All_events_durations', 'Event_type', 'Numerical_Label']).to_frame().T\n",
    "    n = len(input_data_2_seg)\n",
    "    df_repeated = pd.concat([row_df_use] * n, ignore_index=True, axis=0)\n",
    "    \n",
    "    # Concatenate event data with repeated participant details\n",
    "    concatenated_df = pd.concat([input_data_2_seg, df_repeated], axis=1, ignore_index=False)\n",
    "    \n",
    "    # Append processed data to lists based on 'Numerical_Label' value\n",
    "    input_data_2_segs.append(concatenated_df)\n",
    "    new_less.append(input_data_2_seg)\n",
    "    \n",
    "    if row['Numerical_Label'] == 1:\n",
    "        data_frames.append(concatenated_df)\n",
    "        input_data_2_segs_1.append(concatenated_df)  # For input_data_2_1\n",
    "    else:\n",
    "        data_frames_1.append(concatenated_df)\n",
    "        input_data_2_segs_2.append(concatenated_df)  # For input_data_2_2\n",
    "        \n",
    "    data_frames_less.append(input_data_2_seg)\n",
    "\n",
    "# Compile all segments into separate DataFrames for output\n",
    "input_data_2_1 = pd.concat(input_data_2_segs_1, axis=0, ignore_index=True)  # For 'Numerical_Label' == 1\n",
    "input_data_2_2 = pd.concat(input_data_2_segs_2, axis=0, ignore_index=True)  # For 'Numerical_Label' == 0\n",
    "\n",
    "# Generate label arrays for the compiled data lengths\n",
    "input_data_2_1_label = np.full((len(input_data_2_1),), 1)\n",
    "input_data_2_2_label = np.full((len(input_data_2_2),), 0)\n",
    "\n",
    "# At this point:\n",
    "# - input_data_2_1 contains compiled data for 'Numerical_Label' == 1. # AWNS\n",
    "# - input_data_2_2 contains compiled data for 'Numerical_Label' == 0. # AWS\n",
    "# - input_data_2_1_label and input_data_2_2_label provide labels for each set.\n",
    "# - data_frames and data_frames_1 contain separated data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4956f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to divide a DataFrame into segments of a \n",
    "# fixed length, ensuring all segments have the same number of rows\n",
    "\n",
    "def segment_dataframe(df, sample_length):\n",
    "    segments = []\n",
    "    num_segments = len(df) // sample_length\n",
    "    for i in range(num_segments):\n",
    "        start = i * sample_length\n",
    "        end = start + sample_length\n",
    "        segments.append(df.iloc[start:end, :])\n",
    "    if len(df) % sample_length != 0:\n",
    "        # pad the last segment with zeros\n",
    "        segment = df.iloc[end:, :]\n",
    "        padding = pd.DataFrame(0, index=range(sample_length - len(segment)), columns=df.columns)\n",
    "        segments.append(pd.concat([segment, padding]))\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd53e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_length = 20\n",
    "segments = []\n",
    "for i in data_frames:\n",
    "    dd = segment_dataframe(i, sample_length)\n",
    "    #print(len(dd))\n",
    "    segments.append(dd)\n",
    "segments_1 = []\n",
    "for i in data_frames_1:\n",
    "    dd = segment_dataframe(i, sample_length)\n",
    "    #print(len(dd))\n",
    "    segments_1.append(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8fb09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# generate three different random numbers between 0 and len(data_frames)\n",
    "num1_awns, num2_awns, num3_awns = random.sample(range(len(data_frames)), 3)\n",
    "\n",
    "# ensure num2_awns is different from num1_awns\n",
    "while num2_awns == num1_awns:\n",
    "    num2_awns = random.randint(0, len(data_frames)-1)\n",
    "\n",
    "# ensure num3_aws is different from both num1_awns and num2_awns\n",
    "while num3_awns == num1_awns or num3_awns == num2_awns:\n",
    "    num3_awns = random.randint(0, len(data_frames)-1)\n",
    "\n",
    "#print(num1_awns, num2_awns, num3_awns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffad5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate three different random numbers between 0 and len(data_frames_1)-1\n",
    "num5_aws, num6_aws, num7_aws = random.sample(range(len(data_frames_1)), 3)\n",
    "\n",
    "# ensure num6_aws is different from num5_aws\n",
    "while num6_aws == num5_aws:\n",
    "    num6_aws = random.randint(0, len(data_frames_1)-1)\n",
    "\n",
    "# ensure num7_aws is different from num5_aws and num6_aws\n",
    "num7_aws = random.randint(0, len(data_frames_1)-1)\n",
    "while num7_aws == num5_aws or num7_aws == num6_aws:\n",
    "    num7_aws = random.randint(0, len(data_frames_1)-1)\n",
    "\n",
    "#print(num5_aws, num6_aws, num7_aws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e007188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_all = [item for i, sublist in enumerate(segments) for item in sublist if i not in [num1_awns, num2_awns, num3_awns]]\n",
    "#print(len(segments_all)) \n",
    "segments_all_1 = [item for i, sublist in enumerate(segments_1) for item in sublist if i not in [num5_aws, num6_aws, num7_aws]]\n",
    "#print(len(segments_all_1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba308e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make test and validation sets\n",
    "# put num1_awns and num3_aws together # test\n",
    "segments_test = segments[num1_awns]+ segments[num2_awns]+segments_1[num5_aws]+segments_1[num6_aws]\n",
    "#print(len(segments_test)) \n",
    "# put num2_awns and num4_aws together \n",
    "segments_val =  segments[num3_awns]+segments_1[num7_aws]\n",
    "#print(len(segments_val))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d80c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list_seg = segments_all+segments_all_1\n",
    "#len(combined_list_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ccabbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_1_label = np.full((len(segments_all),), 0)\n",
    "input_data_2_label = np.full((len(segments_all_1),), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate((input_data_1_label,input_data_2_label), axis=0)\n",
    "#y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394790ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate labels for test and validation sets \n",
    "# Test , len of awns = 0 is (num1_awns) and len of aws = 1 is (num3_aws)\n",
    "y_test_1 = np.full((len(segments[num1_awns]),), 0)\n",
    "y_test_2 = np.full((len(segments[num2_awns]),), 0)\n",
    "y_test_3 = np.full((len(segments_1[num5_aws]),), 1)\n",
    "y_test_4 = np.full((len(segments_1[num6_aws]),), 1)\n",
    "y_test = np.concatenate((y_test_1,y_test_2,y_test_3,y_test_4), axis=0)\n",
    "#print(y_test.shape)\n",
    "# Val , len of awns = 0 is (num2_awns) and len of aws = 1 is (num4_aws)\n",
    "y_val_1 = np.full((len(segments[num3_awns]),), 0)\n",
    "y_val_2 = np.full((len(segments_1[num7_aws]),), 1)\n",
    "\n",
    "y_val = np.concatenate((y_val_1,y_val_2), axis=0)\n",
    "#print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a74e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= np.array(combined_list_seg).reshape((len(combined_list_seg), sample_length, 14))\n",
    "\n",
    "# print the shape of the resulting array\n",
    "#print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45224b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test= np.array(segments_test).reshape((len(segments_test), sample_length, 14))\n",
    "\n",
    "# print the shape of the resulting array\n",
    "#print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c232ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val= np.array(segments_val).reshape((len(segments_val), sample_length, 14))\n",
    "\n",
    "# print the shape of the resulting array\n",
    "#print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a58dc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(472, 20, 14)\n"
     ]
    }
   ],
   "source": [
    "# Augment the training data set \n",
    "\n",
    "# create 3D array of shape (num_samples, height, width)\n",
    "original_data = X_train\n",
    "\n",
    "# create an empty array to store rotated data\n",
    "rotated_data = np.empty_like(original_data)\n",
    "\n",
    "# loop over each sample in the original data\n",
    "for i in range(original_data.shape[0]):\n",
    "    # rotate the sample by 180 degrees\n",
    "    rotated_sample = np.rot90(original_data[i], k=2, axes=(0, 1))\n",
    "    # store the rotated sample in the rotated data array\n",
    "    rotated_data[i] = rotated_sample\n",
    "\n",
    "# concatenate the original and rotated data along the sample axis\n",
    "augmented_data = np.concatenate((original_data, rotated_data), axis=0)\n",
    "\n",
    "# print the shape of the augmented data\n",
    "print(augmented_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f32515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_aug = np.concatenate((y_train,y_train), axis=0)\n",
    "#y_train_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data is a NumPy array of floats\n",
    "augmented_data = np.array(augmented_data, dtype=np.float32)\n",
    "y_train_aug = np.array(y_train_aug, dtype=np.float32)\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "X_test = np.array(X_test, dtype=np.float32)\n",
    "X_val = np.array(X_val, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47df9eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and run the LSTM model \n",
    "# Model \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=32, dropout=0.2, recurrent_dropout=0.2, input_shape=(None,14)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f16bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "1/1 [==============================] - 1s 697ms/step\n",
      "Accuracy: 0.92\n",
      "Precision: 0.9285714285714286\n",
      "Recall: 0.9285714285714286\n",
      "F1-score: 0.9285714285714286\n",
      "\n",
      "Iteration 2:\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Accuracy: 0.84\n",
      "Precision: 0.7777777777777778\n",
      "Recall: 1.0\n",
      "F1-score: 0.8750000000000001\n",
      "\n",
      "Iteration 3:\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Accuracy: 0.84\n",
      "Precision: 0.7777777777777778\n",
      "Recall: 1.0\n",
      "F1-score: 0.8750000000000001\n",
      "\n",
      "Iteration 4:\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Accuracy: 0.84\n",
      "Precision: 0.7777777777777778\n",
      "Recall: 1.0\n",
      "F1-score: 0.8750000000000001\n",
      "\n",
      "Iteration 5:\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Accuracy: 0.76\n",
      "Precision: 0.75\n",
      "Recall: 0.8571428571428571\n",
      "F1-score: 0.7999999999999999\n",
      "\n",
      "Average Results over 5 Iterations:\n",
      "Average Accuracy: 0.8400000000000001\n",
      "Average Precision: 0.8023809523809524\n",
      "Average Recall: 0.9571428571428571\n",
      "Average F1-score: 0.8707142857142858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "# Repeat the model training and evaluation five times\n",
    "for i in range(5):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    \n",
    "    history = model.fit(augmented_data, y_train_aug, validation_data=(X_val, y_val), epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    y_pred = np.round(y_pred)\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "    print('Accuracy:', accuracy)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1-score:', f1_score)\n",
    "    print()\n",
    "\n",
    "# Calculate averages\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "\n",
    "# Print the average results\n",
    "print(\"Average Results over 5 Iterations:\")\n",
    "print('Average Accuracy:', avg_accuracy)\n",
    "print('Average Precision:', avg_precision)\n",
    "print('Average Recall:', avg_recall)\n",
    "print('Average F1-score:', avg_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cross-validation scores\n",
    "acc = [] # accuracy values - \n",
    "plt.boxplot(acc)\n",
    "num =5\n",
    "plt.title(\"LSTM Accuracy for 5 Different Test Sets ; n = %i\" %num)\n",
    "plt.xticks([])\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the f-1 scores\n",
    "f1 = [] # f1 values - \n",
    "plt.boxplot(f1)\n",
    "num =5\n",
    "plt.title(\"LSTM F-1 Score for 5 Different Test Sets ; n = %i\" %num)\n",
    "plt.xticks([])\n",
    "plt.ylabel(\"F-1 Score\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
