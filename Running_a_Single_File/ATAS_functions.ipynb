{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a8723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_display_file_wf(file_name): # file_name e.g.: 'audiofile.wav'\n",
    "    samplerate, data = wavfile.read(file_name)\n",
    "    # Open WAV file using wave module for additional information\n",
    "    with wave.open(file_name, 'rb') as wave_file:\n",
    "        print(\"Number of channels in file:\", wave_file.getnchannels())\n",
    "        print(\"Samplerate/ data per second:\", wave_file.getframerate())\n",
    "        print(\"Total data in file:\", wave_file.getnframes())\n",
    "        print(\"Total file time:\", wave_file.getnframes() / wave_file.getframerate())    \n",
    "    sr, data = wavfile.read(file_name)\n",
    "    if data.ndim >= 2:\n",
    "        # Extract the first channel if multi-channel\n",
    "        x = data[:, 0]\n",
    "    else:\n",
    "        # If single channel, use the data as is\n",
    "        x = data\n",
    "    return x, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670854c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_file_noise_wf (file_name,x,sr):\n",
    "    reduced_noise = nr.reduce_noise(y=x, sr =sr)\n",
    "    wav_file_nr = file_name.rsplit('.', maxsplit=1)[0] + '_nr.wav'\n",
    "    wavfile.write(wav_file_nr, sr, reduced_noise)\n",
    "    x_nr, sr_nr = librosa.load(wav_file_nr,sr= None)\n",
    "    return x_nr, sr_nr, wav_file_nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_file_trim_wf (wav_file_nr, start_time, end_time):\n",
    "    startTime = start_time*1000 \n",
    "    endTime = end_time*1000 \n",
    "    song = AudioSegment.from_wav(wav_file_nr)\n",
    "    s = int(startTime)           \n",
    "    e = int(endTime)               \n",
    "    extract = song[s:e]\n",
    "    f_name_1 = file_name.rsplit('.', maxsplit=1)[0]\n",
    "    f_trim_file = f_name_1+ str('_nr')+str('_extract.wav')\n",
    "    f_name_2 = f_trim_file.rsplit('.', maxsplit=1)[0]\n",
    "    pp = os.getcwd()\n",
    "    trim_file_viz = pp + '/' + f_name_2 + '.wav'\n",
    "    trim_file = extract.export(f_trim_file, format=\"wav\") \n",
    "    x_nr_ex, sr_nr_ex = librosa.load(trim_file,sr= None)\n",
    "    return x_nr_ex, sr_nr_ex, trim_file, f_trim_file,trim_file_viz, f_name_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab089f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_silence_t(fs, signal, audio_segment, silence_thresh_rms, min_silence_len=50,seek_step=1):\n",
    "    seg_len = len(audio_segment)\n",
    "    if seg_len < min_silence_len:\n",
    "        return []\n",
    "    \n",
    "    silence_starts = []\n",
    "\n",
    "    last_slice_start = seg_len - min_silence_len\n",
    "    slice_starts = range(0, last_slice_start + 1, seek_step)\n",
    "\n",
    "    if last_slice_start % seek_step:\n",
    "        slice_starts = itertools.chain(slice_starts, [last_slice_start])\n",
    "    \n",
    "    slice_starts_use = []\n",
    "    \n",
    "    for j in slice_starts:\n",
    "        slice_starts_use.append(j)\n",
    "\n",
    "    for i, k in zip(slice_starts_use,range(len(slice_starts_use))):\n",
    "\n",
    "        fs_n = fs/1000\n",
    "        sig_i = int(slice_starts_use[k]*fs_n)\n",
    "        sig_j = sig_i+int(50*fs_n)\n",
    "        audio_slice_sig = signal[sig_i:sig_j]\n",
    "\n",
    "        FRAME_LENGTH = 25\n",
    "        HOP_LENGTH = 10\n",
    "        zcr_audio_slice = librosa.feature.zero_crossing_rate(y=audio_slice_sig,frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n",
    "        s_zcr = (np.sum(zcr_audio_slice))\n",
    "\n",
    "        if np.mean(librosa.feature.rms(y=audio_slice_sig, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH))<=silence_thresh_rms and s_zcr == 0:        \n",
    "                silence_starts.append(i)\n",
    "\n",
    "    if not silence_starts:\n",
    "        return []\n",
    "\n",
    "    silent_ranges = []\n",
    "\n",
    "    prev_i = silence_starts.pop(0)\n",
    "    current_range_start = prev_i\n",
    "\n",
    "    for silence_start_i in silence_starts:\n",
    "        continuous = (silence_start_i == prev_i + seek_step)\n",
    "        silence_has_gap = silence_start_i > (prev_i + min_silence_len)\n",
    "\n",
    "        if not continuous and silence_has_gap:\n",
    "            silent_ranges.append([current_range_start,\n",
    "                                  prev_i + min_silence_len])\n",
    "            current_range_start = silence_start_i\n",
    "        prev_i = silence_start_i\n",
    "\n",
    "    silent_ranges.append([current_range_start,\n",
    "                          prev_i + min_silence_len])\n",
    "    return silent_ranges  \n",
    "\n",
    "def detect_nonsilent_t(fs, signal, audio_segment, silence_thresh_rms, min_silence_len=50, seek_step=1):\n",
    "    silent_ranges = detect_silence_t(fs, signal, audio_segment,  silence_thresh_rms,min_silence_len=50,seek_step=1)\n",
    "    len_seg = len(audio_segment)\n",
    "\n",
    "    if not silent_ranges:\n",
    "        return [[0, len_seg]]\n",
    "\n",
    "    if silent_ranges[0][0] == 0 and silent_ranges[0][1] == len_seg:\n",
    "        return []\n",
    "\n",
    "    prev_end_i = 0\n",
    "    nonsilent_ranges = []\n",
    "    \n",
    "    for start_i, end_i in silent_ranges:\n",
    "        nonsilent_ranges.append([prev_end_i, start_i])\n",
    "        prev_end_i = end_i\n",
    "\n",
    "    if end_i != len_seg:\n",
    "        nonsilent_ranges.append([prev_end_i, len_seg])\n",
    "\n",
    "    if nonsilent_ranges[0] == [0, 0]:\n",
    "        nonsilent_ranges.pop(0)\n",
    "\n",
    "    return nonsilent_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_detection_seg_wf(fs, signal, seg_time, trim_file, segment_size_t):\n",
    "    signal_n = signal / (2**15)\n",
    "    signal_len = len(signal_n)\n",
    "    segment_size = int(segment_size_t * fs)  \n",
    "\n",
    "    # Break signal into list of segments \n",
    "    segments = numpy.array([signal_n[x:x + segment_size] for x in numpy.arange(0, signal_len, segment_size)])\n",
    "    no_of_segments = len(segments)\n",
    "\n",
    "    rms_all = []\n",
    "    for s in segments:\n",
    "        rms = librosa.feature.rms(y=s)\n",
    "        rms_1 = np.mean(rms)\n",
    "        rms_all.append(rms_1)\n",
    "    \n",
    "    silence_thresh_rms =  numpy.mean(rms_all) \n",
    "\n",
    "    # Get the time points and trim the file \n",
    "    startTimes, endTimes = seg_time\n",
    "    s = int(startTimes * 1000)\n",
    "    e = int(endTimes * 1000)\n",
    "    \n",
    "    song = AudioSegment.from_wav(trim_file)\n",
    "    extract = song[s:e]\n",
    "\n",
    "    f = 'wav_file_trim_seg'\n",
    "    extract.export(f+'_extract.wav', format=\"wav\") \n",
    "    \n",
    "    pp = os.getcwd()\n",
    "    trim_file_seg = pp + '/' + f + '_extract.wav'\n",
    "    \n",
    "    # Detect pauses\n",
    "    myaudio = intro = AudioSegment.from_wav(trim_file_seg)\n",
    "    signal_t, fs_t = librosa.load(trim_file_seg, sr= None) \n",
    "    \n",
    "    silence_1 = detect_silence_t(fs_t, signal_t, myaudio,  silence_thresh_rms, min_silence_len=50, seek_step=1)\n",
    "    silence_p = np.array([((start/1000),(stop/1000)) for start,stop in silence_1]) #in sec\n",
    "    \n",
    "    #Trim the file so as to remove the correct pauses detected - thus keeping just 'speech' (also speech-time threshold based)\n",
    "    myaudio = intro = AudioSegment.from_wav(trim_file_seg)\n",
    "    silence_2 = detect_nonsilent_t(fs_t, signal_t, myaudio,silence_thresh_rms, min_silence_len=50,seek_step=1)\n",
    "\n",
    "    silence_rem = np.array([((start/1000),(stop/1000)) for start,stop in silence_2]) \n",
    "\n",
    "    return silence_p, silence_rem, silence_thresh_rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392364dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment-wise detection\n",
    "def event_detection_seg_wf_window(signal, fs, segment_size_t_window, trim_file):\n",
    "    signal_n_file = signal / (2**15)\n",
    "    signal_len_file = len(signal_n_file)\n",
    "    segment_size_file = int(segment_size_t_window * fs)\n",
    "\n",
    "    # Break signal into list of segments \n",
    "    segments_file = numpy.array([signal_n_file[x:x + segment_size_file] for x in numpy.arange(0, signal_len_file, segment_size_file)])\n",
    "    no_of_segments_file = len(segments_file)\n",
    "    \n",
    "    seg_sam_len = [len(seg) for seg in segments_file]\n",
    "    seg_sam_len_t = np.cumsum(seg_sam_len)\n",
    "    \n",
    "    seg_time_req = seg_sam_len_t/fs\n",
    "    seg_time_req_1 = np.arange(0, segment_size_t_window*len(segments_file), segment_size_t_window, float)\n",
    "    seg_time = np.vstack((seg_time_req_1,seg_time_req)).T\n",
    "\n",
    "    all_seg_details_silence_p_file = []\n",
    "    all_seg_details_silence_rem_file = []\n",
    "\n",
    "    add_to_pause_speech_detected_time= np.arange(0, segment_size_t_window * len(segments_file), segment_size_t_window, float)\n",
    "    add_to_pause_speech_detected_time[0] = 0.0\n",
    "    \n",
    "    for seg_file, leng in zip(segments_file, range(len(segments_file))):\n",
    "        indv_seg_details_silence_p_file, indv_seg_details_silence_rem_file, silence_thresh_rms = event_detection_seg_wf(fs, seg_file, seg_time[leng],trim_file,segment_size_t)\n",
    "\n",
    "        for j in range(len(indv_seg_details_silence_p_file)):\n",
    "            indv_seg_details_silence_p_file[j] = tuple(y+add_to_pause_speech_detected_time[leng] for y in indv_seg_details_silence_p_file[j])\n",
    "        \n",
    "        for k in range(len(indv_seg_details_silence_rem_file)):\n",
    "            indv_seg_details_silence_rem_file[k] = tuple(x+add_to_pause_speech_detected_time[leng] for x in indv_seg_details_silence_rem_file[k])\n",
    "\n",
    "        all_seg_details_silence_p_file.append(indv_seg_details_silence_p_file)\n",
    "        all_seg_details_silence_rem_file. append(indv_seg_details_silence_rem_file)\n",
    "        \n",
    "    seg_time_leng = seg_time[leng][1]\n",
    "    return all_seg_details_silence_p_file, all_seg_details_silence_rem_file, seg_time_leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6404865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine event data for entire file\n",
    "def detect_proper_events(all_seg_details_silence_p_file,all_seg_details_silence_rem_file,speech_time_threshold,pause_time_threshold, f_name_current, seg_time_leng):\n",
    "    all_seg_details_silence_rem_file = [x for x in all_seg_details_silence_rem_file if x != []]\n",
    "    all_seg_details_silence_p_file = [x for x in all_seg_details_silence_p_file if x != []]\n",
    "    ff = []\n",
    "    for z in range(len(all_seg_details_silence_p_file)):\n",
    "        if all_seg_details_silence_p_file[z] == []:\n",
    "            ff.append(z)\n",
    "    for ind_del_ff in sorted(ff, reverse=True):\n",
    "        del all_seg_details_silence_p_file[ind_del_ff]\n",
    "    silence_p_in = numpy.concatenate( all_seg_details_silence_p_file, axis=0)\n",
    "    len(silence_p_in)\n",
    "    \n",
    "    ff_s = []\n",
    "    for z_s in range(len(all_seg_details_silence_rem_file)):\n",
    "        if all_seg_details_silence_rem_file[z_s] == []:\n",
    "            ff_s.append(z_s)\n",
    "    for ind_del_ff_s in sorted(ff_s, reverse=True):\n",
    "        del all_seg_details_silence_rem_file[ind_del_ff_s]\n",
    "    silence_rem_in = numpy.concatenate( all_seg_details_silence_rem_file, axis=0)\n",
    "    len(silence_rem_in)\n",
    "    \n",
    "    if silence_p_in[-1][1] > silence_rem_in[-1][1]:\n",
    "        silence_p_in[-1][1] = seg_time_leng\n",
    "        \n",
    "    if silence_rem_in[-1][1] > silence_p_in[-1][1]:\n",
    "        silence_rem_in[-1][1] = seg_time_leng\n",
    "        \n",
    "    merged_list_sil_rem = []\n",
    "    for l_1 in silence_rem_in:\n",
    "        for h_1 in l_1: \n",
    "            merged_list_sil_rem.append(h_1)\n",
    "\n",
    "    from collections import Counter\n",
    "    counts_sil_rem = Counter(merged_list_sil_rem)\n",
    "    merged_list_wo_dup = [k_1 for k_1 in merged_list_sil_rem if counts_sil_rem[k_1] == 1]\n",
    "\n",
    "    silence_rem_event_list = np.array(merged_list_wo_dup) \n",
    "    silence_rem= np.reshape(silence_rem_event_list,((int((len(silence_rem_event_list))/2)),2))\n",
    "    \n",
    "    merged_list_sil_p = [] \n",
    "    for l_2 in silence_p_in:\n",
    "        for h_2 in l_2: \n",
    "            merged_list_sil_p.append(h_2)\n",
    "\n",
    "    from collections import Counter\n",
    "    counts_sil_p = Counter(merged_list_sil_p)\n",
    "    merged_list_wo_dup_p = [k_2 for k_2 in merged_list_sil_p if counts_sil_p[k_2] == 1]\n",
    "\n",
    "    silence_p_event_list = np.array(merged_list_wo_dup_p) \n",
    "    silence_p = np.reshape(silence_p_event_list,((int((len(silence_p_event_list))/2)),2))\n",
    "    \n",
    "    sp_s = numpy.asarray(silence_rem)\n",
    "    all_speech_ini = [((sp_n_s[1] - sp_n_s[0]) * 1000) for sp_n_s in sp_s] \n",
    "\n",
    "    speech_del_in= [i for i,v in enumerate(all_speech_ini) if v > speech_time_threshold] \n",
    "    speech_del_chk = [i for i,v in enumerate(all_speech_ini) if v < speech_time_threshold]\n",
    "    \n",
    "    silence_rem_final = silence_rem[speech_del_in]\n",
    "    \n",
    "    sp_p = numpy.asarray(silence_p)\n",
    "    \n",
    "    all_pause_ini = []\n",
    "    all_pause_ini = [((sp_n_p[1] - sp_n_p[0]) * 1000) for sp_n_p in sp_p]\n",
    "    \n",
    "    pause_del_in= [i_p for i_p,v_p in enumerate(all_pause_ini) if v_p > pause_time_threshold] \n",
    "    pause_del_chk= [i_p for i_p,v_p in enumerate(all_pause_ini) if v_p < pause_time_threshold]\n",
    "    \n",
    "    silence_p_final = silence_p[pause_del_in]\n",
    "    \n",
    "    initial_labels_speech = ['s' if i in speech_del_in else 'us' for i in range(len(silence_rem))]\n",
    "    initial_labels_pause = ['p' if j in pause_del_in else 'up' for j in range(len(silence_p))]\n",
    "\n",
    "                \n",
    "    data_s = np.array( list(zip(silence_rem[:,0],silence_rem[:,1],initial_labels_speech)))\n",
    "    data_p = np.array( list(zip(silence_p[:,0],silence_p[:,1],initial_labels_pause)))\n",
    "    \n",
    "    speech_pause_combined = np.concatenate((data_p,data_s))\n",
    "    panda_speech_pause_combined_intial = pd.DataFrame(speech_pause_combined,columns = [\"Time_start\",\"Time_end\", \"Labels\"])\n",
    "    panda_speech_pause_combined_intial.Time_start = panda_speech_pause_combined_intial.Time_start.astype(float)\n",
    "    panda_speech_pause_combined_intial.Time_end = panda_speech_pause_combined_intial.Time_end.astype(float)\n",
    "    panda_speech_pause_combined_intial_u = panda_speech_pause_combined_intial.sort_values('Time_end')\n",
    "    panda_speech_pause_combined_intial_u.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df = panda_speech_pause_combined_intial_u\n",
    "    df_s = df.loc[(df['Labels'] == 's') | (df['Labels'] == 'us')]\n",
    "    \n",
    "    df_s_min = min(df_s.index)\n",
    "    df_s_max = max(df_s.index)\n",
    "    \n",
    "    df= df[df.index > df_s_min-1]\n",
    "    df= df[df.index < df_s_max+1]\n",
    "    \n",
    "    panda_speech_pause_combined_change_label_in = df.reset_index()\n",
    "    data_labels = np.array(panda_speech_pause_combined_change_label_in.Labels)\n",
    "    \n",
    "    all_s = np.where(data_labels == \"s\")\n",
    "    all_p = np.where(data_labels == \"p\")\n",
    "    all_us = np.where(data_labels == \"us\")\n",
    "    all_up = np.where(data_labels == \"up\")\n",
    "    \n",
    "    all_s_and_p = np.sort(numpy.concatenate((all_s[0], all_p[0])))\n",
    "    for i in all_us:\n",
    "        panda_speech_pause_combined_change_label_in = panda_speech_pause_combined_change_label_in.drop(i)\n",
    "        \n",
    "    for i in all_up:\n",
    "        panda_speech_pause_combined_change_label_in = panda_speech_pause_combined_change_label_in.drop(i)\n",
    "        \n",
    "    panda_speech_pause_combined_change_label_in = panda_speech_pause_combined_change_label_in.reset_index()\n",
    "    new_data_labels = panda_speech_pause_combined_change_label_in.Labels\n",
    "    \n",
    "    silence_p_rep =  [i for i in range(len(new_data_labels)) if new_data_labels[i] == 'p']\n",
    "    silence_rem_rep =  [i for i in range(len(new_data_labels)) if new_data_labels[i] == 's']\n",
    "    \n",
    "    panda_speech_pause_combined_change_label_final = panda_speech_pause_combined_change_label_in\n",
    "    panda_speech_pause_combined_change_label_final = panda_speech_pause_combined_change_label_final.assign(Labels=new_data_labels)\n",
    "    \n",
    "    dddf = panda_speech_pause_combined_change_label_final\n",
    "    ddf = dddf\n",
    "    ddf[\"Labels_X\"] = ddf[\"Labels\"].shift()\n",
    "    drop_chk = []\n",
    "    for i in dddf.index:\n",
    "        if i != 0:\n",
    "            if dddf.Labels[i] == dddf.Labels_X[i] :\n",
    "                #print(i) \n",
    "                ddf = ddf.replace(ddf.Time_end[i-1], dddf.Time_end[i])\n",
    "                drop_chk.append(i)\n",
    "                \n",
    "    ddf = ddf.drop(index=drop_chk, axis=0)\n",
    "    ddf[\"Time_diff\"]= ddf[\"Time_end\"]-ddf[\"Time_start\"]\n",
    "\n",
    "    long_p = ddf[(ddf[\"Labels\"] == 'p') & (ddf[\"Time_diff\"] >= 0.15)].index\n",
    "    short_p = ddf[(ddf[\"Labels\"] == 'p') & (ddf[\"Time_diff\"] < 0.15)].index\n",
    "\n",
    "    silence_p_final_end = ddf[ddf[\"Labels\"] == 'p'].index\n",
    "    silence_rem_final_end = ddf[ddf[\"Labels\"] == 's'].index\n",
    "\n",
    "    long_p_durations = ddf.loc[long_p, \"Time_diff\"].tolist()\n",
    "    short_p_durations = ddf.loc[short_p, \"Time_diff\"].tolist()\n",
    "\n",
    "\n",
    "    return silence_p_final_end, silence_rem_final_end, ddf, long_p,short_p, long_p_durations,short_p_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb41db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_S_P_A (trim_file_viz,ddf ):    \n",
    "    x_nr_ex, sr_nr_ex = librosa.load(trim_file_viz,sr= None)\n",
    "    %matplotlib notebook\n",
    "    plt.figure(figsize=(12,5))\n",
    "    for i in ddf.index:\n",
    "        if (ddf[\"Labels\"][i] == 'p') & (ddf[\"Time_diff\"][i] > 0.15):\n",
    "            start = ddf.Time_start[i]\n",
    "            end = ddf.Time_end[i]\n",
    "            plt.axvspan(start, end, color='r',alpha=0.2)\n",
    "        if (ddf[\"Labels\"][i] == 'p') & (ddf[\"Time_diff\"][i] <= 0.15):\n",
    "            start = ddf.Time_start[i]\n",
    "            end = ddf.Time_end[i]\n",
    "            plt.axvspan(start, end, color='blue',alpha=0.2)\n",
    "        if (ddf[\"Labels\"][i] == 's') :\n",
    "            start = ddf.Time_start[i]\n",
    "            end = ddf.Time_end[i]\n",
    "            plt.axvspan(start, end, color='g', alpha=0.2)\n",
    "\n",
    "    red_patch = mpatches.Patch(color='r',alpha=0.2, label='Long Pause (>150 ms)')\n",
    "    blue_patch = mpatches.Patch(color='blue',alpha=0.2, label='Short Pause (<=150 ms)')\n",
    "    green_patch = mpatches.Patch(color='g',alpha=0.2, label='Vocal Event (>100 ms)')\n",
    "    plt.legend(handles=[red_patch,blue_patch,green_patch])\n",
    "\n",
    "    \n",
    "    librosa.display.waveshow(x_nr_ex, sr = sr_nr_ex)\n",
    "    plt.xlabel('Time',fontsize=15)\n",
    "    plt.ylabel('Amplitude',fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790fa9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_statistics_wf(f_name_current, f_trim_file, x_nr_ex, sr_nr_ex,silence_p_final_end, silence_rem_final_end, speech_time_threshold,pause_time_threshold,ddf, long_p, short_p):\n",
    "\n",
    "    file = wave.open(f_name_current)\n",
    "    time_ex = (len(x_nr_ex)/sr_nr_ex)*1000 # total time of clipped file in ms\n",
    "    \n",
    "    all_pauses = [ddf.Time_diff[s] * 1000 for s in silence_p_final_end]\n",
    "    all_speech = [ddf.Time_diff[sp] * 1000 for sp in silence_rem_final_end]\n",
    "    \n",
    "    pause_dur = sum(all_pauses)\n",
    "    speech_dur = sum(all_speech)\n",
    "    \n",
    "    total_time = pause_dur+speech_dur\n",
    "    long_p_len = len(long_p)\n",
    "    short_p_len = len(short_p)\n",
    "    \n",
    "    # Statistical calculations\n",
    "    \n",
    "    Speech_threshold = speech_time_threshold\n",
    "    Pause_threshold = pause_time_threshold\n",
    "    Todal_duration_in_minutes = time_ex/100\n",
    "    percent_speech = (speech_dur/time_ex)*100\n",
    "    percent_pause = (pause_dur/time_ex)*100\n",
    "    Total_Duration_Unclipped_s = (file.getnframes()/file.getframerate())\n",
    "    Total_Duration_Clipped_s = time_ex/1000\n",
    "    speech_duration_s = round(speech_dur/1000,4)\n",
    "    pause_duration_s =  round(pause_dur/1000,4)\n",
    "    speech_events =  len(all_speech)\n",
    "    pause_events = len(all_pauses)\n",
    "    all_speech_s = [x / 1000 for x in all_speech]\n",
    "    all_pauses_s = [x / 1000 for x in all_pauses]\n",
    "    if len(all_speech_s)>0:\n",
    "        mean_speech_s = statistics.mean(all_speech_s)\n",
    "    else:\n",
    "        mean_speech_s = numpy.NaN\n",
    "\n",
    "    if len(all_speech)>1:    \n",
    "        Std_dev_speech_s = statistics.stdev(all_speech_s)\n",
    "        CV_speech_duration_s = variation(all_speech_s)\n",
    "    else: \n",
    "        Std_dev_speech_s = numpy.NaN\n",
    "        CV_speech_duration_s = numpy.NaN\n",
    "    pass\n",
    "\n",
    "    if len(all_pauses_s)>0:\n",
    "        mean_pause_s = statistics.mean(all_pauses_s)\n",
    "    else: \n",
    "        mean_pause_s = numpy.NaN\n",
    "    if len(all_pauses_s)>1:      \n",
    "        Std_dev_pause_s = statistics.stdev(all_pauses_s)\n",
    "        CV_pause_duration_s = variation(all_pauses_s)\n",
    "    else: \n",
    "        Std_dev_pause_s = numpy.NaN\n",
    "        CV_pause_duration_s = numpy.NaN\n",
    "    pass\n",
    "            \n",
    "    data = {'File_Name': [f_name_current],\n",
    "        'Speech Time Threshold_ms': [speech_time_threshold],\n",
    "        'Pause Time Threshold_ms': [pause_time_threshold],\n",
    "        'Percent_Pause': [percent_pause],  \n",
    "        'Percent_Speech': [percent_speech],\n",
    "        'Total_Duration_Unclipped_s': [Total_Duration_Unclipped_s],\n",
    "        'Total_Duration_Clipped_s': [Total_Duration_Clipped_s],\n",
    "        'Speech_Duration_s': [speech_dur/1000],\n",
    "        'Pause_Duration_s': [pause_dur/1000],\n",
    "        'Speech_Events':[speech_events],\n",
    "        'Pause_Events': [pause_events],\n",
    "        'Mean Speech_s': [mean_speech_s],\n",
    "        'Std Dev Speech':[Std_dev_speech_s],\n",
    "        'CV Speech': [CV_speech_duration_s],\n",
    "        'Mean Pause_s':[mean_pause_s],\n",
    "        'Std Dev Pause':[Std_dev_pause_s],\n",
    "        'CV Pause':[CV_pause_duration_s]\n",
    "        }\n",
    "    df = pd.DataFrame(data, columns= ['File_Name',\n",
    "                                      'Speech Time Threshold_ms',\n",
    "                                      'Pause Time Threshold_ms',\n",
    "                                      'Percent_Pause',\n",
    "                                      'Percent_Speech',\n",
    "                                      'Total_Duration_Unclipped_s',\n",
    "                                      'Total_Duration_Clipped_s',\n",
    "                                      'Speech_Duration_s', \n",
    "                                      'Pause_Duration_s', \n",
    "                                      'Speech_Events', \n",
    "                                      'Pause_Events',\n",
    "                                      'Mean Speech_s',\n",
    "                                      'Std Dev Speech',\n",
    "                                      'CV Speech',\n",
    "                                      'Mean Pause_s',\n",
    "                                      'Std Dev Pause',\n",
    "                                      'CV Pause'\n",
    "                                     ])\n",
    "\n",
    "    return df, ddf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
